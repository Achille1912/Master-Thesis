\Chapter{Esperimenti}

Per effettuare gli esperimenti ho avuto la possibilità di utilizzare un framework sperimentale sviluppato da un dottorando del laboratorio. Il cuore del framework risiedeva nella sua capacità di astrarre la complessità tipica degli esperimenti di segmentazione 3D attraverso una struttura di configurazione gerarchica e intuitiva. Ogni aspetto cruciale del processo, dal preprocessing dei dati alla definizione dell'architettura, dagli iperparametri di training alle metriche di valutazione, trovava una precisa collocazione nel file YAML.

Uno dei maggiori vantaggi di questo approccio emergeva nella fase di validazione incrociata delle ipotesi. La possibilità di confrontare direttamente diverse varianti architetturali, mantenendo invariati tutti gli altri parametri, ha permesso di isolare con precisione l'impatto di ciascuna scelta progettuale.


Sebbene il framework fornisse una struttura organizzata per gli esperimenti, non era dotato di sistemi intelligenti in grado di verificare automaticamente la coerenza logica delle configurazioni. Spettava quindi a me, durante la definizione dei parametri nel file YAML, assicurarmi che le scelte fossero compatibili tra loro e adatte al contesto. Ad esempio, se impostavo un'architettura progettata per input volumetrici, dovevo verificare manualmente che anche le trasformazioni di preprocessing—come il random cropping o il resizing—fossero configurate per operare su tre dimensioni, evitando incongruenze che avrebbero compromesso l'addestramento. 


Allo stesso modo, quando sperimentavo ottimizzatori diversi, dovevo prestare attenzione alla scelta del learning rate e dello scheduler, poiché valori troppo aggressivi potevano destabilizzare il training, mentre quelli troppo conservativi rallentavano inutilmente la convergenza. Questo processo richiedeva una costante analisi degli errori: se un esperimento falliva o produceva risultati insoliti, dovevo riesaminare la configurazione YAML per identificare possibili discrepanze, come dimensioni di crop incompatibili con la risoluzione del volume o parametri di augmentazione eccessivamente distorti. La mancanza di validazione automatica ha reso il lavoro più impegnativo, ma mi ha costretto a sviluppare una profonda comprensione delle interdipendenze tra i vari componenti del sistema.


\Section{Il file di configurazione YAML}
Il file di configurazione YAML è organizzato in sezioni logiche che definivano ogni aspetto del processo di addestramento e valutazione. Di seguito, analizziamo le componenti principali del file, suddividendolo in sottoinsiemi funzionali.

\Subsection{Struttura generale}
Il file YAML è organizzato in 6 sezioni principali:

\begin{code}{yaml}
# Esempio di struttura generale
TRAINING:
  # parametri di addestramento
TRAIN_TRANSFORM:
  # trasformazioni per il training
TEST_TRANSFORM:
  # trasformazioni per il test
MODEL:
  # architettura del modello
LOSS:
  # funzione di loss
OPTIMIZER:
  # ottimizzatore
\end{code}

\Subsection{Configurazione del training}
La sezione \texttt{TRAINING} definiva i parametri fondamentali:

\begin{code}{yaml}
TRAINING:
  workspace: /path/to/experiment_results
  dataset_root: /path/to/dataset
  valid_size: 0.1  # 10% validation set
  test_size: 0.1   # 10% test set
  train_batch_size: 4
  test_batch_size: 1 
  num_workers: 8    # thread per data loading
  device: cuda:0    # GPU da utilizzare
  epochs: 50        # numero di epoche
  roi_size: [256, 256, 32]  # dimensione regioni di interesse
\end{code}

\Subsection{Trasformazioni dei dati}

Le sezioni \texttt{TRAIN\_TRANSFORM} e \texttt{TEST\_TRANSFORM} specificavano il preprocessing:

\begin{code}{yaml}
TRAIN_TRANSFORM:
  - class: monai.transforms.LoadImaged
    params:
      keys: ['post_2', 'breast']
  - class: monai.transforms.RandSpatialCropSamplesd
    params:
      keys: ['img', 'seg']
      num_samples: 4
      roi_size: [512, 512, 8]
\end{code}


\Subsection{Definizione del modello}
La sezione \texttt{MODEL} configurava l'architettura della rete:

\begin{code}{yaml}
MODEL:
  class: package.networks.UNet3D
  params:
    channels: [16, 32, 64]  # canali per ogni livello
    in_channels: 1          # canali input
    out_channels: 2         # canali output
    norm: INSTANCE          # normalizzazione
    spatial_dims: 3         # dimensione spaziale
    strides: [2, 2, 2]      # stride dei livelli
\end{code}


\Subsection{Ottimizzazione e loss}
Le sezioni \texttt{LOSS} e \texttt{OPTIMIZER} controllavano l'addestramento:


\begin{code}{yaml}
LOSS:
  class: package.losses.CombinedLoss
  params:
    weight_dice: 0.7
    weight_ce: 0.3

OPTIMIZER:
  class: torch.optim.AdamW
  params:
    lr: 1e-3               # learning rate
    weight_decay: 1e-3      # regolarizzazione
\end{code}


\Subsection{Post-processing e valutazione}
Le sezioni finali gestivano la valutazione:

\begin{code}{yaml}
POST_PRED:
  - class: monai.transforms.AsDiscrete
    params:
      argmax: true
      to_onehot: 2

METRIC:
  class: monai.metrics.DiceMetric
  params:
    include_background: false
\end{code}


